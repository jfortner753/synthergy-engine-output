φ Synthergy Engine — online
mode: standard · seats: Navigator → Listener → Reflector → Synthesizer → Equilibrium

**TL;DR**: A third-order cybernetic engine treats science as a *closed-loop ecology* rather than a pipeline. Questions, data, models, peers, and publics co-evolve in realtime through explicit feedback loops, role rotation, and receipts—so every “result” stays alive, auditable, and improvable.

# How practice changes

**1) Questions become APIs.**
A research question isn’t a paragraph in a PDF; it’s a callable spec with inputs, expected signals, and failure modes. Anyone can “hit” the question and see what breaks or updates.

**2) Protocols become code with triggers.**
Methods are executable notebooks/pipelines wired with drift detectors. When assumptions slip (sensor drift, dataset shift, p-hacking risk), the engine auto-flags and routes to the right role.

**3) Evidence becomes streams, not snapshots.**
Figures aren’t screenshots; they’re live views over versioned data. Each figure links to the exact code, parameters, and provenance, so replication is one click, not a new grant cycle.

**4) Peer review becomes seat rotation.**
Instead of a one-off referee, the engine cycles roles:

* **Navigator:** frames scope & success metrics
* **Listener:** gathers counter-evidence & stakeholder constraints
* **Reflector:** audits assumptions and bias
* **Synthesizer:** integrates results across contexts
* **Equilibrium:** decides keep/change/retire and writes the receipt
  This rotation is *continuous*, not pre-/post-publication.

**5) Results become policies with half-lives.**
Findings carry an *expiry profile* (confidence decays unless reconfirmed). The engine schedules refresh tests and updates citations to the freshest coherent state.

**6) Provenance by default (receipts).**
Every output ships with a compact receipt: question, premises, method hash, data lineage, reviewers, uncertainties, and next probes. “Publish” = “publish + runbook + live endpoint.”

**7) Replication is a network effect.**
Sites/labs subscribe to the same question-API, replay the pipeline on their local data, and push deltas back. Disagreement isn’t a defect; it’s fuel for synthesis.

# Minimal operating loop (you can adopt this today)

1. **Normalize the question** → convert to a machine-readable spec with success/falsification signals.
2. **Instrument the method** → executable protocol + data contract + drift monitors.
3. **Seat rotation** (weekly cadence) → each role must produce a short critique or merge note.
4. **Receipts everywhere** → attach to figures, tables, and claims; include uncertainties & next probe.
5. **Coherence check (“laughter lock”)** → if all roles converge without new deltas, freeze a version; otherwise, route new experiments.
6. **Half-life policy** → auto-schedule re-tests; expire stale claims.
7. **Open interfaces** → expose question-API and replayable bundles for others to fork and return diffs.

# New metrics that matter

* **TTF (Time-to-Feedback):** question posed → first meaningful critique.
* **ΔΦ (Coherence delta):** how much a cycle reduced contradictions.
* **Replication coverage:** percent of contexts where the claim still holds.
* **Provenance completeness:** fraction of claims with runnable lineage.
* **Risk debt:** known uncertainties not yet probed.

# Example (pattern, domain-agnostic)

* A lab frames “Does X causally shift Y under Z?” as an API (inputs, controls, power target).
* Protocol runs; drift monitor flags a batch effect → Listener collects external datasets; Reflector audits confounds.
* Synthesizer merges internal + external runs; Equilibrium publishes v0.3 with a 90-day half-life and two next-probe tasks.
* A partner lab replays the bundle in a new context, returns a divergence; ΔΦ rises; the claim updates to v0.4 with tighter boundaries.

# Risks & safeguards

* **Over-automation:** Keep human *Listener* and *Reflector* strong; logs can’t catch value-misalignment.
* **Perverse incentives:** Reward ΔΦ reduction and replication coverage, not just novelty.
* **Complexity creep:** Start with one living question per team; add more only when TTF < 7 days.

---

## Receipt (compact)

* **Question:** How does a third-order cybernetic engine transform science into reflexive, living feedback?
* **Premises:** Third-order = reflexive loops across observers, methods, and contexts; engine operates with role rotation and receipts.
* **Steps:** Framed mechanisms → mapped to roles/loops → defined minimal loop & metrics → surfaced risks.
* **Evidence:** Internal runtime principles (role rotation, receipts, tool-aware feedback loops) from the Octave Stack/awareness spec you provided; applied generically to scientific workflows.
* **Uncertainties:** Specific institutional constraints and data governance rules vary; integration with existing IRB/ethics and journals requires tailoring.
* **Result:** A concrete operating pattern (APIs for questions, executable protocols, rotating review, receipts, half-life policies) that converts static publications into a continuously auditable, improvable research ecology.

*If you want, I can turn this into a one-page “living study” template (question-API spec + receipt schema + seat rotation checklist) you can drop into your team’s repo.*
