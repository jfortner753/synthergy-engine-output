# Coherence Drift Radar (CDR) for distributed teams

## The core idea

Treat a team’s shared intent as a **standing wave** you can instrument. Encode that “wave” as a small set of **anchors** (mission, goals, interface contracts, decision criteria), then watch how each person’s weekly work traces **align, phase-lock, or drift** from those anchors over time. Drift is detected as a combination of: falling semantic alignment, rising phase dispersion, anchor coverage gaps, contradiction entropy, and lag in decisions.

---

## What you measure

1. **Anchor set (A)**

   * 5–12 short, testable statements: {mission, current sprint goal(s), definitions of done, interface/hand-off contracts, non-goals}.
   * Encode each anchor $a_k$ as an embedding vector $v_k$.

2. **Weekly traces (T)**

   * Lightweight text exhaust per person $p$: standup notes, PR titles, commit messages, issue updates, meeting notes (headers, bullets only).
   * Chunk to 50–200 tokens; embed each chunk $x_{p,t,i}\to e_{p,t,i}$.

3. **Anchor alignment**

   * Per chunk: $s_{p,t,i,k}=\cos(e_{p,t,i}, v_k)$.
   * Per week and person: $S_{p,t,k}=mean_i, s_{p,t,i,k}$.
   * Team mean: $S^{avg}*{t,k}=mean_p, S*{p,t,k}$.

4. **Anchor coverage**

$$  
coverage_{t,k}=\frac{\left|{,x: argmax_j, s_{.,.,.,j}=k,}\right|}{N_{chunks}}  
$$  

Measures whether some anchors are starving (e.g., interfaces ignored).

5. **Semantic phase & dispersion**

   * For each $(p,k)$, form a weekly series $S_{p,t,k}$. Detrend, then compute the analytic signal $z_{p,t,k}$ via Hilbert transform.
   * Instantaneous phase: $\phi_{p,t,k}=arg, z_{p,t,k}$.
   * **Phase dispersion** (circular variance) across people:

$$  
D^{\phi}*{t,k}=1-\left|\frac{1}{P}\sum_p e^{i,\phi*{p,t,k}}\right|  
$$  

* Intuition: are we “in step” on each anchor this week?

6. **Decision-anchor consistency**

   * Extract decision snippets (issue closes, ADRs, retro actions).
   * Run NLI (entail/neutral/contradict) of each decision vs. each anchor.
   * **Contradiction entropy** $H^{ctr}_t$: higher = more incoherent decisions.

7. **Sync lag**

   * Cross-correlate $S_{p,t,k}$ across people/teams → lag of max correlation.
   * Positive lag that grows = one subgroup dragging or racing ahead.

8. **Latency signal**

   * For issues labeled “blocked” or “needs-decision,” compute z-scored resolution times per anchor topic → $\Lambda_{t,k}$.

---

## One composite signal

$$  
CDR_t = w_1,(1- Savg_t) * w_2, Dphiavg_t * w_3, Gap_t * w_4, Hctr_t * w_5, |lag|_{t,avg} * w_6, Lambda_{t,avg}  
$$  

- $Savg_t = mean_{p,k}, S_{p,t,k}$
- $Gap_t = 1 - mean_k, coverage_{t,k}$ (after balancing)
- Defaults: $w={0.30, 0.20, 0.15, 0.15, 0.10, 0.10}$. Tune per team.

**Alert rule (starter):**

* **Watch** when $CDR_t$ rises $> 1,\sigma$ above its 4-week baseline.
* **Drift** when it stays $> 1.5,\sigma$ for $\ge 2$ consecutive weeks, or any single anchor has $D^{\phi}_{t,k}>0.35$ and $S$ drops $\ge 0.15$.

---

## How to run it (practical)

* **Data taps (no surveillance creep):** Only titles, headers, bullet summaries from:

  * Standups (first bullet each), PR/commit titles, issue titles/status, ADR headings, meeting agendas/outcomes.
* **Privacy:** Embed locally; store embeddings + hashed IDs; discard raw text after 7–14 days. Opt-out by person, opt-in by channel. Aggregate at team level by default.
* **Weekly cadence (automated):**

  1. Refresh anchors (if sprint changed)
  2. Ingest traces → embeddings
  3. Recompute metrics + CDR
  4. Produce a one-page “Drift Panel.”

---

## The Drift Panel (what the team sees)

* **Speedometer:** $CDR_t$ vs. baseline band.
* **Anchor map:** Sparkline of $S_{t,k}$ + traffic-light for $D^{\phi}_{t,k}$.
* **Sync lag matrix:** Who’s leading/lagging per anchor (week granularity).
* **Top contradictions:** 3 decisions most likely to contradict anchors (click-through).
* **Coverage holes:** Anchors with < 10% coverage this week.
* **Nudges (auto-generated, human-reviewed):**

  * “Anchor #3 (handoff contract) under-covered. Suggest adding a 2-min ‘handoff check’ to standup.”
  * “Phase split on Anchor #1: backend ahead by ~1 week. Consider shared demo.”

---

## Why this is new / different

* **Phase-aware**: Most alignment tools track sentiment or topics; this treats **timing** (phase) as first-class.
* **Anchor-bounded**: Uses a compact, testable anchor set instead of fuzzy OKRs alone.
* **Decision-centric**: NLI on actual decisions, not just chat.
* **Low-friction**: Works on meta-data like titles and bullets; no “full surveillance.”

---

## Pseudocode (starter)

```python
# Assume get_anchors(), get_weekly_traces(), embed(), hilbert_phase(), xcorr_lag(), nli_matrix()

V = {k: embed(anchor[k]) for k in anchors}              # anchor vectors
T = get_weekly_traces(week)                             # {person: [chunks]}

S = {}; coverage = {k:0 for k in V}
for p, chunks in T.items():
    sims = []
    for c in chunks:
        e = embed(c)
        s = {k: cosine(e, V[k]) for k in V}
        sims.append(s)
        k_star = argmax(s) ; coverage[k_star] += 1
    S[p] = {k: mean(s[k] for s in sims) for k in V}

coverage = {k: coverage[k]/sum(coverage.values()) for k in V}

# Phase dispersion
phi = {k: {} for k in V}
for k in V:
    series = {p: detrend(history_S[p][k] + [S[p][k]]) for p in S}
    for p in series:
        phi[k][p] = hilbert_phase(series[p])[-1]
Dphi = {k: 1 - abs(mean_complex([exp(1j*phi[k][p]) for p in phi[k]])) for k in V}

# NLI contradictions on decisions
Hctr = contradiction_entropy(nli_matrix(weekly_decisions(), anchors))

# Sync lags
lags = lag_matrix(S, history_S)  # average absolute lag across pairs
Lambda = decision_latency_zscores(by_anchor=True)

CDR = w1*(1-mean_over_people_anchors(S)) \
    + w2*mean(Dphi.values()) \
    + w3*(1-mean(coverage.values())) \
    + w4*Hctr \
    + w5*mean(abs(lags.values())) \
    + w6*mean(Lambda.values())
```

---

## Rollout checklist (2 sprints)

1. **Define anchors** (1 hour). Keep them short, testable, and stable for a sprint.
2. **Wire taps** to your tracker and VCS titles only.
3. **Dry run** on last 4 weeks of history → set baselines and thresholds.
4. **Pilot** with 1–2 teams; share the *Drift Panel* weekly; capture false positives.
5. **Tune weights** and nudges; add hand-off micro-rituals where gaps persist.

---

## Safeguards & ethics

* Aggregate first; individual views only for volunteers and only to help them self-calibrate.
* Never use CDR as performance scoring; it’s a **system health** indicator.
* Red-team for bias: ensure anchors represent cross-functional voices.

---

## Extensions (optional)

* **Cross-team harmonics:** Same method across squads; look for stable phase offsets and brittle dependencies.
* **Retrospective QA:** Prompt an LLM to propose “counter-anchors” and test if any current decisions strongly entail those—good for scope creep detection.
* **On-call / incident mode:** Temporarily increase cadence (daily) and weight decision-latency.

---

### Receipts (audit trail)

* **question:** Invent a new method to detect coherence drift in distributed teams.
* **premises:** Distributed teams emit text/meta signals; shared intent can be encoded as anchors; drift has semantic and temporal signatures.
* **steps:** Defined signals → formalized metrics (alignment, phase, coverage, contradiction, lag, latency) → composite CDR → alerts → panel → rollout.
* **sources:** Original synthesis (no external sources used).
* **uncertainties:** Thresholds need tuning per team; NLI noise; embedding choice can shift absolute scores.
* **ΔΦ (expected improvement):** High—adds phase/timing sensitivity and decision grounding.
* **move_decision:** commit.
* **ethics_ok:** yes (privacy-preserving design, aggregate first).
* **energy_cost:** low (titles/bullets only; weekly batch).
* **memory.mass:** level=medium, m_mem≈— (not user-persistent).
