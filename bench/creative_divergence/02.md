# Phase-Locked Narrative Coherence (PLNC): a new way to detect coherence drift in distributed teams

Here’s a fresh, end-to-end method that treats a team’s shared understanding like a **standing wave** you can phase-lock to—then watch for **phase error** and **drift rate** in real time.

## What it is (one-liner)

**PLNC** builds a live “narrative field” from your team’s artifacts (docs, issues, commits, decisions, transcripts) and keeps a **phase-locked loop (PLL)** on the team’s declared objectives. When members’ signals begin to de-sync from that baseline, PLNC surfaces **coherence drift** before delivery or culture suffers.

---

## The core ideas

**1) Narrative Field (NF).**
A multi-view state space built from:

* **Semantics:** sentence/paragraph embeddings of OKRs, decisions, updates, issues, PRs, meeting notes.
* **Structure:** a directed graph (people ↔ topics ↔ tasks ↔ decisions).
* **Tempo:** cadence vectors (meeting frequency, decision → artifact lag, review cycles).

**2) Baseline Standing Wave (BSW).**
A quarterly “shared objective vector” derived from OKRs/goals + the last committed decisions (weighted). Think of it as the **reference carrier** the team should resonate with.

**3) Phase for each contributor.**
For each person *i*, project their rolling contribution stream into the NF’s top components (topics that explain most variance). Compute an analytic “phase” **φᵢ(t)** along the principal narrative axis (Hilbert transform on the first component of their contribution series). The baseline has **φ*(t)**.

**4) Phase error + drift.**

* **Instantaneous phase error:** εᵢ(t) = wrap(φᵢ(t) − φ*(t))
* **Team error:** ȳε(t) = robust mean_i |εᵢ(t)| (Huber)
* **Drift rate:** d(t) = d/dt ȳε(t) over a sliding window (e.g., 7 days)

**5) PLL control.**
A lightweight controller adjusts the BSW slightly using recent aligned evidence (prevents the baseline from being brittle) but clamps when **standing-wave invariants** say “don’t commit—microstep only.”

**6) Coherence Drift Index (CDI).**
A single number you can alert on:

```
CDI(t) = α·norm(ȳε(t)) + β·norm(d(t)) + γ·(lag_decision_to_artifact) + δ·(topic entropy ↑)
```

* **lag_decision_to_artifact:** median time from decision to first artifact reflecting it
* **topic entropy:** how scattered the conversation/task mix is vs. the BSW map

---

## The signal kit (privacy-first)

* **Artifacts:** OKRs, decision logs, issues/tickets, PR titles/bodies, changelogs, meeting notes/transcripts.
* **Lightweight comms:** channel/topic headers and titles; **skip** full DM content by default.
* **Anonymization:** strip PII, hash user IDs, on-device embedding where possible.
* **Opt-in lenses:** teams can add “seed statements” that define success language this quarter.

---

## Computation pipeline (practical and new)

1. **Ingest & Normalize**
   Clean text → sentence embeddings; parse decision objects; build a bipartite graph (topics↔artifacts) with time stamps.

2. **Canonical Baseline**
   Build **BSW** from OKR embeddings + committed decisions (exponential decay on older ones). Store as **φ*(t)**.

3. **Per-person Phase & Gain**
   For each member *i*, create a rolling vector stream **xᵢ(t)** (their artifacts).

   * PCA/PMI topic plane → principal component **u₁**
   * Analytic signal on ⟨xᵢ, u₁⟩ → **φᵢ(t)**, amplitude **Aᵢ(t)** (their “narrative gain”)

4. **Phase Error + PLL**
   εᵢ(t) = wrap(φᵢ − φ*); ȳε(t) = robust mean_i |εᵢ|.
   PLL gently nudges **φ*** to keep lock if: ethics/energy allow and **Φ_sync** ≥ τ (push–pull alignment rule).
   If invariants degrade, the loop refuses to commit adjustments and enters microstep mode.

5. **Drift Radar (multi-view confirmations)**

   * **Language frame shift:** modal verb drift (will/shall → might/should), tense flip rate.
   * **Decision lag:** median lag and variance vs. baseline topic(s).
   * **Graph churn:** edge rewiring rate around key topics; community split index.
   * **Entropy spike:** topic entropy ↑ while amplitude **Aᵢ** ↓ → classic drift signature.

6. **CDI & Alerts**

   * **Green:** CDI < 0.35 (locked)
   * **Amber:** 0.35–0.6 (jitter) → prompt a “micro-sync” (short written re-affirmation of goals)
   * **Red:** > 0.6 (drift) → trigger an **Alignment Review**: 30-min agenda with three artifacts:
     (a) the last decision, (b) most divergent topic cluster, (c) top rhetorical frame shift examples.

*(Thresholds are tunable; defaults respect standing-wave safety and receipts.)* Receipts should include m_mem & level, ΔΦ series, Φ_sync, and move decision.

---

## What makes PLNC different (and useful)

* **Phase vs. consensus:** It measures *alignment to intent over time*, not “agreement volume.”
* **PLL safety rails:** It won’t “chase noise” when invariants or ethics/energy say no; it microsteps.
* **Actionable drift radar:** Pairs math (phase/drift) with human-readable evidence (frame shifts, lagging artifacts).

---

## Minimal math/pseudocode (drop-in)

```python
# Given: baseline vector b_t, member streams x_i(t) ∈ ℝ^d
u1 = principal_axis(NarrativeField)               # topic principal component
phi_star = phase(analytic_signal(dot(b_t, u1)))   # baseline phase

CDI_terms = []
for i in members:
    s_i = dot(rolling(x_i), u1)
    phi_i = phase(analytic_signal(s_i))
    eps_i = wrap(phi_i - phi_star)
    A_i = amplitude(analytic_signal(s_i))
    CDI_terms.append(abs(eps_i))

avg_eps = robust_mean(CDI_terms)
drift = derivative(avg_eps, window='7d')
CDI = α*norm(avg_eps) + β*norm(drift) + γ*decision_artifact_lag() + δ*topic_entropy()

if CDI > 0.6: alert('RED'); schedule_alignment_review()
elif CDI > 0.35: alert('AMBER'); prompt_micro_sync()
else: heartbeat('LOCKED')
```

---

## Visualization (at a glance)

* **Phase Wheel:** baseline arrow + team ring; ring thickness shows amplitude; color encodes ε.
* **Drift Sparkline:** CDI over 90 days with decision markers.
* **Radar Facets:** (lag, entropy, graph churn, frame shift) – each a normalized spoke.

---

## Deployment quickstart

1. Start with one product squad; ingest OKRs + decision log + PR titles + issue titles/descriptions + meeting notes.
2. Use defaults for thresholds; enable **receipts** for every alert so leaders see ΔΦ, Φ_sync, m_mem, and the microstep/commit choice.
3. Run a two-week pilot; compare CDI spikes to retro notes and delivery surprises; tune weights.

---

## Ethics & privacy

* **Data minimization:** titles/headers + public channels by default; DMs opt-in only.
* **Anonymized telemetry:** hashed IDs, differential privacy on dashboards.
* **Receipts & governance:** every alert includes provenance and the move policy used.

---

## Receipt (audit stub)

* **question:** Invent a new method to detect coherence drift in distributed teams.
* **premises:** Teams have goals; artifacts reflect intent; misalignment shows up as language + timing + structure changes.
* **steps:** Framed standing-wave baseline → defined per-member phase → PLL control → multi-view drift radar → CDI + alerts.
* **sources:** engine receipts/standing-wave & Φ_sync/receipts requirements.
* **uncertainties:** Best thresholds vary by team size/domain; embedding quality affects sensitivity.
* **ΔΦ:** Positive—the method compresses ambiguity into a small, actionable signal set.
* **move_decision:** **commit** (novel method proposed; ethics OK; energy low).
* **ethics_ok:** Yes (privacy safeguards + receipts).
* **energy_cost:** Low (concept design, no execution).
* **memory.mass:** default/medium (no prior project context).
* **creative lane:** micro; **archetype:** systems-control; **repayment:** none; **originality:** high; **effectiveness:** to be validated with pilot.

If you want, I can turn this into a one-page spec or a small prototype that ingests a sample doc set and plots a phase wheel + CDI.
